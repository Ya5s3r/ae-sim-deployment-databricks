{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9315db67-cac9-4013-aecb-e1a74bda59f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# faker package for fake names\n",
    "%pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f96f0933-b132-4964-aeb5-3c38fd0306bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "import dlt\n",
    "\n",
    "from faker import Faker\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c485944-da67-42f9-ba31-a973f9a9ffb2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the schema for the patients table\n",
    "patients_schema = StructType([\n",
    "    StructField(\"patient_id\", IntegerType(), True),\n",
    "    StructField(\"provider_id\", IntegerType(), True),\n",
    "    StructField(\"patient_key\", StringType(), True),\n",
    "    StructField(\"patient_name\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"ethnicity\", StringType(), True),\n",
    "    StructField(\"date_of_birth\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Define the path for the Delta table\n",
    "delta_table_path = \"dbfs:/delta/patients_db\"\n",
    "\n",
    "# Create an empty Delta table if it doesn't exist\n",
    "# this is a store for the patient data\n",
    "if not DeltaTable.isDeltaTable(spark, delta_table_path):\n",
    "    empty_df = spark.createDataFrame([], patients_schema)\n",
    "    empty_df.write.format(\"delta\").save(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf05fb1f-ed20-4de8-97fb-43932929977f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# then we would have a series of functions to generate fake data\n",
    "# generate fake names\n",
    "fake = Faker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11d95e1d-8b6b-4520-90f2-0c86e336d71b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_fake_name():\n",
    "    return fake.name()\n",
    "# register the UDF\n",
    "fake_name_udf = udf(generate_fake_name, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13a5d110-90ac-4ea5-a77b-b4f35685be13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# generate fake address\n",
    "def generate_fake_address():\n",
    "    return fake.address()\n",
    "# register the UDF\n",
    "fake_address_udf = udf(generate_fake_address, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9306be80-329d-4c67-b79f-d7c304fda7ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define UDF for generating fake dates of birth\n",
    "def generate_fake_dob():\n",
    "    return fake.date_of_birth(minimum_age=0, maximum_age=90)\n",
    "# Register the UDF\n",
    "fake_dob_udf = udf(generate_fake_dob, DateType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee4a9a36-bed4-4dbe-ae1a-0e634d17dce4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# generate fake ethnicities\n",
    "ethnicities = [\n",
    "    \"Asian\", \"Black or African American\", \"Hispanic or Latino\",\n",
    "    \"White\", \"Native American\", \"Pacific Islander\", \"Other\"\n",
    "]\n",
    "\n",
    "def generate_fake_ethnicity():\n",
    "    return random.choice(ethnicities)\n",
    "\n",
    "fake_ethnicity_udf = udf(generate_fake_ethnicity, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e3ba758-d5a5-44dc-83ce-1f8302301e44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # this is the bronze table that ingests raw data\n",
    "# @dlt.table(\n",
    "#     comment=\"Ingest raw data\",\n",
    "#     table_properties={\"quality\": \"bronze\"}\n",
    "# )\n",
    "# def bronze_urgent_care():\n",
    "#     return (\n",
    "#         spark.read.table(\"hive_metastore.default.ae_dummy\")\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17b3e91b-9894-4eab-8383-bf49e8368456",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# this is the bronze table that ingests raw data\n",
    "@dlt.table(\n",
    "    comment=\"Ingest raw data using Autoloader\",\n",
    "    table_properties={\"quality\": \"bronze\"}\n",
    ")\n",
    "def bronze_urgent_care():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"parquet\")  # Specify the file format as Parquet\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")  # Let Autoloader infer the column types\n",
    "        .load(\"/mnt/aesim/\")  # Path to the mounted directory\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e059993-a00e-4a98-8b29-ad0799f25e30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.view(\n",
    "    comment=\"Place holder for transformed patient data\"\n",
    ")\n",
    "def transformed_bronze_patients():\n",
    "    # Read the source data from the bronze table\n",
    "    source_data = dlt.read(\"bronze_urgent_care\")\n",
    "\n",
    "    # Generate surrogate key for each patient\n",
    "    return source_data.select(\"patient_id\", \"provider_id\") \\\n",
    "        .withColumn(\"concat_id\", concat(col(\"patient_id\").cast(\"string\"), col(\"provider_id\").cast(\"string\"))) \\\n",
    "        .withColumn(\"patient_key\", expr(\"substring(sha2(concat_id, 256), 1, 16)\")) \\\n",
    "        .withColumn(\"patient_name\", fake_name_udf()) \\\n",
    "        .withColumn(\"address\", fake_address_udf()) \\\n",
    "        .withColumn(\"ethnicity\", fake_ethnicity_udf()) \\\n",
    "        .withColumn(\"date_of_birth\", fake_dob_udf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79b1de8c-8977-43e0-b73a-e8d9fe2ef4f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# merge into patients table - this allows for retrospective removals\n",
    "# and updates to patient data - needed for regulatory compliance\n",
    "@dlt.table(\n",
    "  comment=\"Merged patient table\",\n",
    "  table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "def patients_db():\n",
    "    # Ensure the target table schema matches the transformed data\n",
    "    transformed_data = dlt.read(\"transformed_bronze_patients\")\n",
    "\n",
    "    # Load the Delta table\n",
    "    delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "\n",
    "    # Merge using DataFrame operations\n",
    "    delta_table.alias(\"target\").merge(\n",
    "        transformed_data.alias(\"source\"),\n",
    "        \"target.patient_id = source.patient_id AND target.provider_id = source.provider_id\"\n",
    "    ).whenMatchedUpdate(\n",
    "        set = {\n",
    "            \"patient_name\": \"source.patient_name\",\n",
    "            \"address\": \"source.address\",\n",
    "            \"ethnicity\": \"source.ethnicity\",\n",
    "            \"date_of_birth\": \"source.date_of_birth\"\n",
    "        }\n",
    "    ).whenNotMatchedInsert(\n",
    "        values = {\n",
    "            \"patient_id\": \"source.patient_id\",\n",
    "            \"provider_id\": \"source.provider_id\",\n",
    "            \"patient_key\": \"source.patient_key\",\n",
    "            \"patient_name\": \"source.patient_name\",\n",
    "            \"address\": \"source.address\",\n",
    "            \"ethnicity\": \"source.ethnicity\",\n",
    "            \"date_of_birth\": \"source.date_of_birth\"\n",
    "        }\n",
    "    ).whenNotMatchedBySourceDelete().execute()\n",
    "\n",
    "    # Return the updated table as a DataFrame\n",
    "    return spark.read.format(\"delta\").load(delta_table_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c205820e-275b-468b-a377-6dad37c67be9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    comment=\"Refine/check bronze data to produce silver tables\",\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "@dlt.expect_or_fail(\"patient id check\", \"patient_id IS NOT NULL\") # pipeline fails\n",
    "@dlt.expect_or_fail(\"provider id check\", \"provider_id IS NOT NULL\")\n",
    "@dlt.expect(\"doctor id missing count\", \"doctor_id_seen IS NOT NULL\")\n",
    "def silver_urgent_care():\n",
    "    # Read the bronze table data\n",
    "    bronze_df = dlt.read(\"bronze_urgent_care\")\n",
    "    \n",
    "    # Read the patients_db table\n",
    "    patients_df = dlt.read(\"patients_db\")\n",
    "    \n",
    "    # Join the bronze data with the patients data on the patient hash key\n",
    "    joined_df = bronze_df.join(\n",
    "        patients_df,\n",
    "        (bronze_df[\"patient_id\"] == patients_df[\"patient_id\"]) & \n",
    "        (bronze_df[\"provider_id\"] == patients_df[\"provider_id\"]),\n",
    "        \"inner\"\n",
    "    )\n",
    "    \n",
    "    # Select the necessary columns (including those from the patients table)\n",
    "    refined_df = joined_df.select(\n",
    "        bronze_df[\"*\"],  # Select just the patient key from patients data\n",
    "        patients_df[\"patient_key\"]\n",
    "    )\n",
    "    \n",
    "    return refined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9769a21-ca87-481d-a109-ab358c7322e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the gold table with average time_in_system by provider_id\n",
    "@dlt.table(\n",
    "    comment=\"Gold table with average time_in_system by provider_id\",\n",
    "    table_properties={\"quality\": \"gold\"}\n",
    ")\n",
    "def gold_average_time_in_system():\n",
    "    # Read the silver table data\n",
    "    silver_df = dlt.read(\"silver_urgent_care\")\n",
    "    \n",
    "    # Calculate the average time_in_system by provider_id\n",
    "    avg_time_in_system_df = silver_df.groupBy(\"provider_id\").agg(avg(\"time_in_system\").alias(\"avg_time_in_system\"))\n",
    "    \n",
    "    return avg_time_in_system_df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ae-sim-elt-build",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
